# -*- coding: utf-8 -*-
"""suc_candle_cpu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BhU4WZWzIca7V-ZgUaqkDtYCWeLJxLMi
"""





"""https://huggingface.github.io/candle/guide/installation.html"""

!git clone https://github.com/huggingface/candle.git

# Commented out IPython magic to ensure Python compatibility.
# %cd candle

!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y --default-toolchain nightly

import os
os.environ['PATH'] += ":/root/.cargo/bin"

!rustc --version
!cargo --version

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/candle/candle-examples

"""https://github.com/huggingface/candle/tree/main/candle-examples/examples/phi"""

cargo run --example phi --release -- --model 2 \
  --prompt "A skier slides down a frictionless slope of height 40m and length 80m. What's the skier speed at the bottom?"

! cargo run --example phi --release -- --model 2 \
  --prompt "A skier slides down a frictionless slope of height 40m and length 80m. What's the skier speed at the bottom?"

! cargo run --example phi --release -- --model 2 \
  --prompt "Who is Napoleon Bonaparte?"

Running on CPU, to run on GPU, build this example with `--features cuda`

!cargo fix --lib -p candle-transformers

"""اشرح بالعربى warning: enum `PositionEmbeddingType` is never used --> candle-transformers/src/models/debertav2.rs:44:6 | 44 | enum PositionEmbeddingType { | ^^^^^^^^^^^^^^^^^^^^^ | = note: `#[warn(dead_code)]` (part of `#[warn(unused)]`) on by default warning: `candle-transformers` (lib) generated 1 warning Finished `dev` profile [unoptimized + debuginfo] target(s) in 1m 26s
بالتأكيد، إليك شرح للرسالة التحذيرية باللغة العربية:

warning: enum PositionEmbeddingType is never used تحذير: التعداد PositionEmbeddingType لم يتم استخدامه أبدًا

هذا يعني أن هناك تعدادًا (enum) باسم PositionEmbeddingType تم تعريفه في الملف candle-transformers/src/models/debertav2.rs في السطر 44، ولكن لم يتم استخدامه فعليًا في أي جزء من الكود.

= note: #[warn(dead_code)] (part of #[warn(unused)]) on by default = ملاحظة: التحذير #[warn(dead_code)] (وهو جزء من التحذير #[warn(unused)]) مُفعّل بشكل افتراضي

تشير هذه الملاحظة إلى أن هذا النوع من التحذيرات (الكود غير المستخدم أو "الكود الميت") مُفعّل تلقائيًا في إعدادات بناء المشروع.

warning: candle-transformers (lib) generated 1 warning تحذير: المكتبة candle-transformers أصدرت تحذيرًا واحدًا

توضح هذه الرسالة أن عملية بناء المكتبة المسماة candle-transformers نتج عنها تحذير واحد.

باختصار، التحذير يعني أن هناك جزءًا من الكود تم تعريفه ولكنه غير مستخدم، وهذا ليس خطأ يمنع تشغيل البرنامج، ولكنه يُعتبر ممارسة جيدة في البرمجة لإزالة الكود غير المستخدم للحفاظ على الكود نظيفًا وسهل الفهم.
"""

!cargo run --example rwkv --release -- --prompt "The smallest prime is "

!cargo run --example rwkv --release -- --prompt "The smallest prime is "

!cargo run --example rwkv --release -- --prompt "Napoleon Bonaparte is "



/content/candle/candle-examples/examples/vit/german-shepherd.webp


/content/candle/candle-examples/examples/yolo-v8/assets/bike.jpg

!cargo run --example vit --release -- --image /content/candle/candle-examples/examples/yolo-v8/assets/bike.jpg

!cargo run --example quantized --release -- --prompt "The best thing about coding in rust is "

!wget https://arxiv.org/pdf/1706.03762.pdf
!cargo run --features cuda,pdf2image --release --example colpali -- --prompt "What is Positional Encoding" --pdf "1706.03762.pdf"

#!wget https://arxiv.org/pdf/1706.03762.pdf
!cargo run --features cpu,pdf2image --release --example colpali -- --prompt "What is Positional Encoding" --pdf "1706.03762.pdf"

#!wget https://arxiv.org/pdf/1706.03762.pdf
!cargo run --features pdf2image --release --example colpali -- --prompt "What is Positional Encoding" --pdf "1706.03762.pdf"

import os
os.environ['PATH'] += ":/root/.cargo/bin"

!rustc --version
!cargo --version

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/candle
!cargo run --features pdf2image --release --example colpali -- --prompt "What is Positional Encoding" --pdf "1706.03762.pdf"

















import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

def generate_prompt(instruction, input=""):
    instruction = instruction.strip().replace('\r\n','\n').replace('\n\n','\n')
    input = input.strip().replace('\r\n','\n').replace('\n\n','\n')
    if input:
        return f"""Instruction: {instruction}

Input: {input}

Response:"""
    else:
        return f"""User: hi

Assistant: Hi. I am your assistant and I will provide expert full response in full details. Please feel free to ask any question and I will always answer it.

User: {instruction}

Assistant:"""

# Load the model on the CPU
model = AutoModelForCausalLM.from_pretrained("RWKV/rwkv-5-world-3b", trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained("RWKV/rwkv-5-world-3b", trust_remote_code=True)

text = "hi"
prompt = generate_prompt(text)

inputs = tokenizer(prompt, return_tensors="pt")
output = model.generate(inputs["input_ids"], max_new_tokens=11, do_sample=True, temperature=1.0, top_p=0.3, top_k=0, )
print(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))

!pip install bitsandbytes

!pip install flash-rwkv

!pip install ninja

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

def generate_prompt(instruction, input=""):
    instruction = instruction.strip().replace('\r\n','\n').replace('\n\n','\n')
    input = input.strip().replace('\r\n','\n').replace('\n\n','\n')
    if input:
        return f"""Instruction: {instruction}

Input: {input}

Response:"""
    else:
        return f"""User: hi

Assistant: Hi. I am your assistant and I will provide expert full response in full details. Please feel free to ask any question and I will always answer it.

User: {instruction}

Assistant:"""

# Load the model on the CPU
model = AutoModelForCausalLM.from_pretrained("RWKV/rwkv-5-world-3b", trust_remote_code=True, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained("RWKV/rwkv-5-world-3b", trust_remote_code=True)

text = "hi"
prompt = generate_prompt(text)

inputs = tokenizer(prompt, return_tensors="pt")
output = model.generate(inputs["input_ids"], max_new_tokens=11, do_sample=True, temperature=1.0, top_p=0.3, top_k=0, )
print(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

def generate_prompt(instruction, input=""):
    instruction = instruction.strip().replace('\r\n','\n').replace('\n\n','\n')
    input = input.strip().replace('\r\n','\n').replace('\n\n','\n')
    if input:
        return f"""Instruction: {instruction}

Input: {input}

Response:"""
    else:
        return f"""User: hi

Assistant: Hi. I am your assistant and I will provide expert full response in full details. Please feel free to ask any question and I will always answer it.

User: {instruction}

Assistant:"""


model = AutoModelForCausalLM.from_pretrained("RWKV/rwkv-5-world-3b", trust_remote_code=True).to(torch.float32)
tokenizer = AutoTokenizer.from_pretrained("RWKV/rwkv-5-world-3b", trust_remote_code=True)

text = "请介绍北京的旅游景点"
prompt = generate_prompt(text)

inputs = tokenizer(prompt, return_tensors="pt")
output = model.generate(inputs["input_ids"], max_new_tokens=333, do_sample=True, temperature=1.0, top_p=0.3, top_k=0, )
print(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

def generate_prompt(instruction, input=""):
    instruction = instruction.strip().replace('\r\n', '\n').replace('\n\n', '\n')
    input = input.strip().replace('\r\n', '\n').replace('\n\n', '\n')
    if input:
        return f"""Instruction: {instruction}

Input: {input}

Response:"""
    else:
        return f"""User: hi

Assistant: Hi. I am your assistant and I will provide expert full response in full details. Please feel free to ask any question and I will always answer it.

User: {instruction}

Assistant:"""

model = AutoModelForCausalLM.from_pretrained("RWKV/rwkv-5-world-3b", trust_remote_code=True).to(torch.float32)
tokenizer = AutoTokenizer.from_pretrained("RWKV/rwkv-5-world-3b", trust_remote_code=True, padding_side='left', pad_token="<s>")

texts = ["请介绍北京的旅游景点", "介绍一下大熊猫", "乌兰察布"]
prompts = [generate_prompt(text) for text in texts]

inputs = tokenizer(prompts, return_tensors="pt", padding=True)
outputs = model.generate(inputs["input_ids"], max_new_tokens=128, do_sample=True, temperature=1.0, top_p=0.3, top_k=0, )

for output in outputs:
    print(tokenizer.decode(output.tolist(), skip_special_tokens=True))

# Load model directly
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("RWKV/rwkv-5-world-3b", trust_remote_code=True, torch_dtype="auto"),